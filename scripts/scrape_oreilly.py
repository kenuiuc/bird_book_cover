#!/usr/bin/env python3
"""
Asynchronous scraper for O'Reilly Animal Menagerie 'bird' pages.

Replaces the original synchronous scraper with an asyncio + aiohttp implementation
to fetch list pages and book pages concurrently for much faster scraping.

Creates ../bird_gallery_static.md with up to N books (default 216).

Usage:
  python3 scripts/scrape_oreilly.py --max 216 --concurrency 12

Notes:
- Uses aiohttp for async HTTP requests and BeautifulSoup for HTML parsing.
- Be polite: default concurrency is moderate and a short delay is applied per request.
"""
import argparse
import asyncio
from urllib.parse import urljoin
from pathlib import Path
from typing import List, Tuple

import aiohttp
from bs4 import BeautifulSoup


LIST_PAGES = [f"https://www.oreilly.com/animals.csp?x-o={i*20}&x-search=bird&x-sort=newest" for i in range(25)]

HEADERS = {"User-Agent": "bird-gallery-async-scraper/1.0 (+https://example.local)"}


async def fetch_text(session: aiohttp.ClientSession, url: str, timeout: int = 20) -> str:
    async with session.get(url, timeout=timeout) as resp:
        resp.raise_for_status()
        return await resp.text()


def extract_book_links(list_html: str, base_url: str = "https://www.oreilly.com") -> List[Tuple[str, str]]:
    soup = BeautifulSoup(list_html, 'html.parser')
    links: List[Tuple[str, str]] = []
    for a in soup.find_all('a', href=True):
        href = a['href']
        if '/library/view/' in href:
            full = urljoin(base_url, href)
            text = (a.get_text() or '').strip()
            links.append((full, text))
    # preserve order while deduping
    seen = set(); out: List[Tuple[str,str]] = []
    for u,t in links:
        if u not in seen:
            seen.add(u); out.append((u,t))
    return out


def extract_cover_and_title(book_html: str, book_url: str):
    soup = BeautifulSoup(book_html, 'html.parser')
    og = soup.find('meta', property='og:image') or soup.find('meta', attrs={'name':'og:image'})
    cover = og['content'] if og and og.get('content') else None
    if not cover:
        imgs = soup.find_all('img')
        for img in imgs:
            src = img.get('src') or img.get('data-src') or ''
            s = (src or '').lower()
            if 'cover' in s or 'oreilly' in s or 'book' in s:
                cover = urljoin(book_url, src)
                break
    title = None
    t = soup.find('title')
    if t and t.string:
        title = t.string.strip()
    if (not title or len(title) < 3):
        h1 = soup.find(['h1','h2'])
        if h1 and h1.get_text():
            title = h1.get_text().strip()
    return cover, title


def render_markdown(items, out_path):
    # items: list of dict {title, cover, url}
    p = Path(out_path).resolve()
    p.parent.mkdir(parents=True, exist_ok=True)
    with p.open('w', encoding='utf8') as f:
        f.write('# O\'Reilly Bird Gallery (static snapshot)\n\n')
        f.write('This static snapshot was generated by scripts/scrape_oreilly.py and contains up to the configured number of books. Images are embedded by URL (no images were downloaded).\n\n')
        f.write('<div style="display:grid; grid-template-columns:repeat(auto-fill,minmax(180px,1fr)); gap:12px;">\n')
        for it in items:
            title = it.get('title') or ''
            cover = it.get('cover')
            url = it.get('url')
            f.write('<div style="border:1px solid #ddd;padding:8px;border-radius:6px;background:#fff;">')
            if cover:
                f.write(f'<a href="{url}" target="_blank" rel="noopener noreferrer"><img src="{cover}" alt="{title}" style="width:100%;height:260px;object-fit:cover;border-radius:4px;" loading="lazy"></a>')
            else:
                f.write(f'<div style="width:100%;height:260px;background:#f6f6f6;display:flex;align-items:center;justify-content:center;color:#999;border-radius:4px;">No cover</div>')
            f.write(f'<div style="margin-top:8px;font-size:0.95rem;color:#222"><a href="{url}" target="_blank" rel="noopener noreferrer">{title}</a></div>')
            f.write('</div>\n')
        f.write('</div>\n')


async def gather_list_pages(session: aiohttp.ClientSession, concurrency: int = 6):
    sem = asyncio.Semaphore(concurrency)
    async def _fetch(url):
        async with sem:
            try:
                text = await fetch_text(session, url)
                print(f'  {url} -> ok')
                return url, text
            except Exception as e:
                print('Failed fetching', url, e)
                return url, None
    tasks = [asyncio.create_task(_fetch(u)) for u in LIST_PAGES]
    results = await asyncio.gather(*tasks)
    return results


async def fetch_book(session: aiohttp.ClientSession, url: str, sem: asyncio.Semaphore, delay: float = 0.0, fallback_text: str = None):
    """Fetch a single book page. On success return dict with url/cover/title.
    On failure return None (so caller can omit the book entirely).
    """
    async with sem:
        try:
            html = await fetch_text(session, url)
            cover, title = extract_cover_and_title(html, url)
            # use fallback text when title not found
            if not title:
                title = fallback_text or url
            # omit if no cover found
            if not cover:
                print('  no cover, skipping', url)
                return None
            return {'url': url, 'cover': cover, 'title': title}
        except Exception as e:
            print('  failed', url, e)
            return None
        finally:
            if delay:
                await asyncio.sleep(delay)


async def main_async(max_items: int, out: str, concurrency: int):
    connector = aiohttp.TCPConnector(limit_per_host=concurrency, ssl=True)
    timeout = aiohttp.ClientTimeout(total=60)
    async with aiohttp.ClientSession(headers=HEADERS, connector=connector, timeout=timeout) as session:
        print('Fetching list pages...')
        list_results = await gather_list_pages(session, concurrency=min(6, concurrency))
        book_links = []
        for url, text in list_results:
            if text:
                links = extract_book_links(text)
                book_links.extend(links)

        # dedupe preserving order
        seen = set(); uniq = []
        for u,t in book_links:
            if u not in seen:
                seen.add(u); uniq.append((u,t))
        print(f'Found {len(uniq)} unique book links; limiting to {max_items}')

        sem = asyncio.Semaphore(concurrency)
        tasks = []
        for i,(url,text) in enumerate(uniq[:max_items], start=1):
            print(f'[{i}/{min(len(uniq),max_items)}] Scheduling {url}')
            tasks.append(fetch_book(session, url, sem, delay=0.05, fallback_text=text))

        raw_results = await asyncio.gather(*tasks)
        # filter out failed/omitted results (None)
        items = [r for r in raw_results if r is not None]

        print(f'Collected {len(items)} items (after filtering failed/no-cover pages)')

        out_path = Path(__file__).parent.joinpath(out).as_posix()
        print('Writing', out_path)
        render_markdown(items, out_path)
        print('Done.')


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--max', type=int, default=216, help='Maximum number of books to include (default 216)')
    parser.add_argument('--out', default='../bird_gallery_static.md', help='Output markdown path (relative to scripts/)')
    parser.add_argument('--concurrency', type=int, default=12, help='Number of concurrent requests (default 12)')
    args = parser.parse_args()
    asyncio.run(main_async(args.max, args.out, args.concurrency))


if __name__ == '__main__':
    main()
