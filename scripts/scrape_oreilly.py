#!/usr/bin/env python3
"""
Asynchronous scraper for O'Reilly Animal Menagerie 'bird' pages.

Replaces the original synchronous scraper with an asyncio + aiohttp implementation
to fetch list pages and book pages concurrently for much faster scraping.

Creates ../bird_gallery_static.md with up to N books (default 216).

Usage:
  python3 scripts/scrape_oreilly.py --max 216 --concurrency 12

Notes:
- Uses aiohttp for async HTTP requests and BeautifulSoup for HTML parsing.
- Be polite: default concurrency is moderate and a short delay is applied per request.
"""
import argparse
import asyncio
from urllib.parse import urljoin
from pathlib import Path
from typing import List, Tuple, Optional, Dict

import aiohttp
from bs4 import BeautifulSoup


LIST_PAGES = [f"https://www.oreilly.com/animals.csp?x-o={i*20}&x-search=bird&x-sort=newest" for i in range(25)]

HEADERS = {"User-Agent": "bird-gallery-async-scraper/1.0 (+https://example.local)"}


async def fetch_text(session: aiohttp.ClientSession, url: str, timeout: int = 20) -> str:
    async with session.get(url, timeout=timeout) as resp:
        resp.raise_for_status()
        return await resp.text()


def extract_book_entries(list_html: str, base_url: str = "https://www.oreilly.com") -> List[Dict[str, Optional[str]]]:
    """Extract book link entries (url, link_text, animal) from a list/search page.

    Bug fix: Animal names actually reside on the list (search results) pages, not on
    the individual book detail pages. Previously we attempted to parse the
    <p class="animal-name"> tag from the book page (often missing), resulting in
    blank animal fields. We now capture the animal name while scanning the list page
    and propagate it as a hint when fetching the book page.
    """
    soup = BeautifulSoup(list_html, 'html.parser')
    entries: List[Dict[str, Optional[str]]] = []
    for a in soup.find_all('a', href=True):
        href = a['href']
        if '/library/view/' not in href:
            continue
        full = urljoin(base_url, href)
        text = (a.get_text() or '').strip()

        # Heuristic: climb a few ancestor levels looking for a p.animal-name
        animal = None
        container = a
        for _ in range(5):  # limit climb depth
            if container is None:
                break
            animal_tag = container.find('p', class_='animal-name') if container else None
            if animal_tag and animal_tag.get_text(strip=True):
                animal = animal_tag.get_text(strip=True)
                break
            container = container.parent

        entries.append({'url': full, 'text': text, 'animal': animal})

    # Deduplicate by URL preserving first occurrence (with its animal/text)
    seen = set(); out: List[Dict[str, Optional[str]]] = []
    for e in entries:
        u = e['url']
        if u not in seen:
            seen.add(u); out.append(e)
    return out


def extract_cover_title_animal(book_html: str, book_url: str):
    """Return (cover, title, animal) from a book detail page.

    Note: Animal name is often not present on the book page itself. We still try
    to extract it (legacy behavior) but usually rely on the list-page provided
    hint captured earlier.
    """
    soup = BeautifulSoup(book_html, 'html.parser')
    og = soup.find('meta', property='og:image') or soup.find('meta', attrs={'name':'og:image'})
    cover = None
    if og and hasattr(og, 'get'):
        cover = og.get('content')
    if not cover:
        imgs = soup.find_all('img')
        for img in imgs:
            src = img.get('src') or img.get('data-src') or ''
            s = (src or '').lower()
            if 'cover' in s or 'oreilly' in s or 'book' in s:
                cover = urljoin(book_url, src)
                break

    title = None
    t = soup.find('title')
    if t:
        try:
            # prefer .get_text() for robustness
            extracted = t.get_text(strip=True)
            if extracted:
                title = extracted
        except Exception:
            pass
    if (not title or len(title) < 3):
        h1 = soup.find(['h1','h2'])
        if h1 and h1.get_text():
            title = h1.get_text().strip()

    animal = None
    p = soup.find('p', class_='animal-name')
    if p:
        animal = p.get_text().strip()

    return cover, title, animal


def render_markdown(items, out_path):
    # items: list of dict {title, cover, url}
    p = Path(out_path).resolve()
    p.parent.mkdir(parents=True, exist_ok=True)
    with p.open('w', encoding='utf8') as f:
        f.write('# O\'Reilly Bird Gallery (static snapshot)\n\n')
        f.write('This static snapshot was generated by scripts/scrape_oreilly.py and contains up to the configured number of books. Images are embedded by URL (no images were downloaded).\n\n')
        f.write('<div style="display:grid; grid-template-columns:repeat(auto-fill,minmax(180px,1fr)); gap:12px;">\n')
        for it in items:
            title = it.get('title') or ''
            cover = it.get('cover')
            url = it.get('url')
            animal = it.get('animal') if it.get('animal') else None
            f.write('<div style="border:1px solid #ddd;padding:8px;border-radius:6px;background:#fff;">')
            if cover:
                f.write(f'<a href="{url}" target="_blank" rel="noopener noreferrer"><img src="{cover}" alt="{title}" style="width:100%;height:260px;object-fit:cover;border-radius:4px;" loading="lazy"></a>')
            else:
                f.write('<div style="width:100%;height:260px;background:#f6f6f6;display:flex;align-items:center;justify-content:center;color:#999;border-radius:4px;">No cover</div>')
            # Text block: animal first (larger), then book title (smaller)
            f.write('<div style="margin-top:8px;">')
            if animal:
                f.write(f'<div style="font-size:1.05rem;font-weight:600;line-height:1.1;color:#1a1a1a;">{animal}</div>')
            f.write(
                f'<div style="font-size:0.9rem;line-height:1.2;">'
                f'<a href="{url}" target="_blank" rel="noopener noreferrer" style="color:#0366d6;text-decoration:underline;">{title}</a>'
                f'</div>'
            )
            f.write('</div>')  # end text block
            f.write('</div>\n')
        f.write('</div>\n')


async def gather_list_pages(session: aiohttp.ClientSession, concurrency: int = 6):
    sem = asyncio.Semaphore(concurrency)
    async def _fetch(url):
        async with sem:
            try:
                text = await fetch_text(session, url)
                print(f'  {url} -> ok')
                return url, text
            except Exception as e:
                print('Failed fetching', url, e)
                return url, None
    tasks = [asyncio.create_task(_fetch(u)) for u in LIST_PAGES]
    results = await asyncio.gather(*tasks)
    return results


async def fetch_book(session: aiohttp.ClientSession, url: str, sem: asyncio.Semaphore, delay: float = 0.0,
                     fallback_text: Optional[str] = None, animal_hint: Optional[str] = None):
    """Fetch a single book page. On success return dict with url/cover/title.
    On failure return None (so caller can omit the book entirely).
    """
    async with sem:
        try:
            html = await fetch_text(session, url)
            cover, title, animal = extract_cover_title_animal(html, url)
            # use fallback text when title not found
            if not title:
                title = fallback_text or url
            # prefer already captured animal hint if page lacked it
            if not animal:
                animal = animal_hint
            # omit if no cover found
            if not cover:
                print('  no cover, skipping', url)
                return None
            return {'url': url, 'cover': cover, 'title': title, 'animal': animal}
        except Exception as e:
            print('  failed', url, e)
            return None
        finally:
            if delay:
                await asyncio.sleep(delay)


async def main_async(max_items: int, out: str, concurrency: int):
    connector = aiohttp.TCPConnector(limit_per_host=concurrency, ssl=True)
    timeout = aiohttp.ClientTimeout(total=60)
    async with aiohttp.ClientSession(headers=HEADERS, connector=connector, timeout=timeout) as session:
        print('Fetching list pages...')
        list_results = await gather_list_pages(session, concurrency=min(6, concurrency))
        book_entries = []
        for url, text in list_results:
            if text:
                entries = extract_book_entries(text)
                book_entries.extend(entries)

        print(f'Found {len(book_entries)} unique book entries; limiting to {max_items}')

        sem = asyncio.Semaphore(concurrency)
        tasks = []
        for i, entry in enumerate(book_entries[:max_items], start=1):
            url = entry['url']; text = entry.get('text'); animal_hint = entry.get('animal')
            print(f'[{i}/{min(len(book_entries),max_items)}] Scheduling {url}')
            tasks.append(fetch_book(session, url, sem, delay=0.05, fallback_text=text, animal_hint=animal_hint))

        raw_results = await asyncio.gather(*tasks)
        # filter out failed/omitted results (None)
        items = [r for r in raw_results if r is not None]

        print(f'Collected {len(items)} items (after filtering failed/no-cover pages)')

        out_path = Path(__file__).parent.joinpath(out).as_posix()
        print('Writing', out_path)
        render_markdown(items, out_path)
        print('Done.')


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--max', type=int, default=216, help='Maximum number of books to include (default 216)')
    parser.add_argument('--out', default='../bird_gallery_static.md', help='Output markdown path (relative to scripts/)')
    parser.add_argument('--concurrency', type=int, default=12, help='Number of concurrent requests (default 12)')
    args = parser.parse_args()
    asyncio.run(main_async(args.max, args.out, args.concurrency))


if __name__ == '__main__':
    main()
